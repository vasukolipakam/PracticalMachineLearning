---
title: "PracticalMachineLearning Course Project"
author: "V.Kolipakam"
date: "February 9, 2016"
output: html_document
---

INTRODUCTION
============

Devices such as Jawbone Up, Nike Fuelband and Fitbit help us collect large amount of data without much expense.One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this 
Practical machine Learning course project we will be using data from accelerometers on the belt, forearm,arm and dumbell of six research particpiants.
Our training data consists of accelerometer data and a label identifying the quality of the activity the participant was doing. Our testing data consists of accelerometer data without the identifying label.The goal  of this project is to predict the labels for the test set observations.The code is given below with decriptions.


Loading Data:
=============
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
library(knitr)


set.seed(12345)

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))

Now lets divide /partion this training set into two

```
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)


Cleaning data :
================

Now the next step would be to remove near zero variance variables.

nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]

nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]

from this now lest remove the first coloumn of the myTraining data set

myTraining <- myTraining[c(-1)]

After being cleaned the variables with more than 60% NA

trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
    if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
                trainingV3 <- trainingV3[ , -j]
            }   
        } 
    }
}

myTraining <- trainingV3
rm(trainingV3)

Now the next step is transforming the testing and myTesting data sets


clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  
# the classe column is removed.

myTesting <- myTesting[clean1]        
# only the variables that are in my Training are allowed in myTesting.

testing <- testing[clean2]           
# only variables that are in myTraining are allowed testing in testing.

dim(myTesting)

dim(testing)

Now getting the data into the same data type

for (i in 1:length(testing) ) {
    for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}

This is done so that testing and myTraining have the same class.

testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]

Now lets use different methods of predicting to see what happens 

Prediction with decision trees :
--------------------------------

set.seed(12345)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)

predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree


plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))


Prediction with random forests:
-------------------------------

set.seed(12345)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf


plot(modFitB1)

plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))

Results of test Data :
-----------------------
Between the two prediction models Random forests had an accuracy of 99.8% which was better than the result from the decision tree which was only 87%.
The expected out-of-sample error is 100-99.89 = 0.11%.


predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2

Now lets write these results to a text file so that it can be submitted

pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}






